{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V100",
   "collapsed_sections": [
    "Ki8KIdXG3VG8",
    "DRWHrWCOQtHS"
   ],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "e323fc445422434a88336c4eeb5d423b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b79a3b4a2ed84b15828ad59b029a5fdd",
       "IPY_MODEL_d2fc80860b234546bc5c0067c585fe91",
       "IPY_MODEL_2c1bcd22eedd4c67be4e962005482dfd"
      ],
      "layout": "IPY_MODEL_36b4457dc0614826882aa714f02dcfe4"
     }
    },
    "b79a3b4a2ed84b15828ad59b029a5fdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb6a1537f6f1401a9d95f8f665440daf",
      "placeholder": "​",
      "style": "IPY_MODEL_685f8f88ebed426f97a8d1a3c3799b5e",
      "value": "Loading pipeline components...: 100%"
     }
    },
    "d2fc80860b234546bc5c0067c585fe91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_874f536d05074145af6a1f866ccfc80d",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b71b767e97e844c1bd3aed1c3a9a456e",
      "value": 7
     }
    },
    "2c1bcd22eedd4c67be4e962005482dfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_379887bc2d3b4547a9bca6fea6071db2",
      "placeholder": "​",
      "style": "IPY_MODEL_094033e01e174fd1881f64d4f5359f00",
      "value": " 7/7 [00:01&lt;00:00,  6.59it/s]"
     }
    },
    "36b4457dc0614826882aa714f02dcfe4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb6a1537f6f1401a9d95f8f665440daf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "685f8f88ebed426f97a8d1a3c3799b5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "874f536d05074145af6a1f866ccfc80d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b71b767e97e844c1bd3aed1c3a9a456e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "379887bc2d3b4547a9bca6fea6071db2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "094033e01e174fd1881f64d4f5359f00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Setup (may take a few minutes)\n",
    "\n",
    "from pathlib import Path\n",
    "import torch.utils.checkpoint\n",
    "import itertools\n",
    "from accelerate import Accelerator\n",
    "from datasets import prompt_dataset\n",
    "from diffusers.pipelines import AutoPipelineForText2Image\n",
    "import utils\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.transforms.functional as TF\n",
    "import os\n",
    "\n",
    "import shutil"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Hyperparameters"
   ],
   "metadata": {
    "id": "cm-dEEZDQ6Ba"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "counting_model_name: str = 'clip-count' #@param ['clip','clip-count'] {type:\"string\"}\n",
    "\n",
    "clazz: str = \"oranges\" #@param {type:\"string\"}\n",
    "amount: int = 10 #@param {type:\"number\"}\n",
    "\n",
    "_lambda: float = 5  #@param {type:\"number\"}\n",
    "scale: float = 60 # 80  #@param {type:\"number\"}\n",
    "is_dynamic_scale_factor: bool = True #@param {type:\"boolean\"}\n",
    "yolo_threshold: float = 0.5 #@param {type:\"number\"}\n",
    "\n",
    "# Affect training time\n",
    "early_stopping: int = 15 #@param {type:\"integer\"}\n",
    "num_train_epochs: int = 50 #@param {type:\"integer\"}\n",
    "\n",
    "# affect variability of the training images\n",
    "# i.e., also sets batch size with accumulation\n",
    "epoch_size: int = 1 #@param {type:\"integer\"}\n",
    "number_of_prompts: int = 1 #@param {type:\"integer\"}\n",
    "batch_size: int = 1 #@param {type:\"integer\"}\n",
    "gradient_accumulation_steps: int = 1 #@param {type:\"integer\"}\n",
    "\n",
    "# Skip if there exists a token checkpoint\n",
    "skip_exists: bool = False #@param {type:\"boolean\"}\n",
    "\n",
    "# Train and Optimization\n",
    "lr: float = 0.1 #@param {type:\"number\"}\n",
    "betas1: tuple = 0.9 #@param {type:\"number\"}\n",
    "betas2: tuple = 0.999 #@param {type:\"number\"}\n",
    "betas = (betas1, betas2)\n",
    "\n",
    "\n",
    "weight_decay: float = 1e-2 #@param {type:\"number\"}\n",
    "eps: float = 1e-08 #@param {type:\"number\"}\n",
    "max_grad_norm: str = \"1\" #@param {type:\"string\"}\n",
    "seed: int = 35 #@param {type:\"integer\"}\n",
    "\n",
    "# Generative model\n",
    "guidance_scale: int = 7 #@param {type:\"integer\"}\n",
    "height: int = 512 #@param {type:\"integer\"}\n",
    "width: int = 512 #@param {type:\"integer\"}\n",
    "num_of_SD_inference_steps: int = 35 #@param {type:\"integer\"}\n",
    "num_of_SD_backpropagation_steps: int = 1 #@param {type:\"integer\"}\n",
    "\n",
    "# Discriminative tokens\n",
    "placeholder_token: str = \"newcls\" #@param {type:\"string\"}\n",
    "initializer_token: str = \"some\" #@param {type:\"string\"}\n",
    "\n",
    "# Path to save all outputs to\n",
    "output_path: str = \"results\" #@param {type:\"string\"}\n",
    "save_as_full_pipeline: bool = True #@param {type:\"boolean\"}\n",
    "\n",
    "# Cuda related\n",
    "device: str = \"cuda\" #@param {type:\"string\"}\n",
    "mixed_precision: str = \"no\" #@param [\"fp16\", \"fp32\"] {type:\"string\"}\n",
    "gradient_checkpointing: bool = True #@param {type:\"boolean\"}\n",
    "\n",
    "# evaluate\n",
    "test_size: int = 3 #@param {type:\"integer\"}"
   ],
   "metadata": {
    "id": "uY7BoAjxz5LD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import namedtuple\n",
    "# Define the configuration names\n",
    "config_names = [\n",
    "    \"counting_model_name\",\n",
    "    \"early_stopping\",\n",
    "    \"num_train_epochs\",\n",
    "    \"epoch_size\",\n",
    "    \"number_of_prompts\",\n",
    "    \"batch_size\",\n",
    "    \"gradient_accumulation_steps\",\n",
    "    \"skip_exists\",\n",
    "    \"betas\",\n",
    "    \"lr\",\n",
    "    \"eps\",\n",
    "    \"weight_decay\",\n",
    "    \"seed\",\n",
    "    \"max_grad_norm\",\n",
    "    \"guidance_scale\",\n",
    "    \"height\",\n",
    "    \"width\",\n",
    "    \"num_of_SD_inference_steps\",\n",
    "    \"num_of_SD_backpropagation_steps\",\n",
    "    \"placeholder_token\",\n",
    "    \"initializer_token\",\n",
    "    \"output_path\",\n",
    "    \"save_as_full_pipeline\",\n",
    "    \"device\",\n",
    "    \"mixed_precision\",\n",
    "    \"gradient_checkpointing\",\n",
    "    \"test_size\",\n",
    "    \"scale\",\n",
    "    \"is_dynamic_scale_factor\",\n",
    "    \"yolo_threshold\",\n",
    "    \"clazz\",\n",
    "    \"amount\"\n",
    "]\n",
    "\n",
    "# Use globals() to extract values from matching variable names\n",
    "config_values = [globals()[name] for name in config_names]\n",
    "\n",
    "# Create the named tuple\n",
    "Config = namedtuple(\"Config\", config_names)\n",
    "config = Config(*config_values)\n",
    "config"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SR2klWJI9c-a",
    "outputId": "614c9805-210c-40db-d56b-e0d576faf618"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Train"
   ],
   "metadata": {
    "id": "Ki8KIdXG3VG8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import YolosForObjectDetection, YolosImageProcessor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# Predict Scale Factor help functions\n",
    "def load_image(img):\n",
    "    if isinstance(img, str) and os.path.isfile(img):\n",
    "        # img is a file path, open with PIL.Image.open()\n",
    "        return Image.open(img)\n",
    "    elif isinstance(img, torch.Tensor):\n",
    "        # img is a tensor, convert to PIL image\n",
    "        transform_to_pil = transforms.ToPILImage()\n",
    "        return transform_to_pil(img.squeeze())\n",
    "    else:\n",
    "        raise ValueError(\"The provided input is neither a valid file path nor a tensor.\")\n",
    "\n",
    "def run_yolo(model, image_processor, image, clazz, threshold=0.4):\n",
    "    count = 0\n",
    "    # image = Image.open(image)\n",
    "    image = load_image(image)\n",
    "\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # print results\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold, target_sizes=target_sizes)[0]\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        if model.config.id2label[label.item()] == clazz:\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "def extract_clip_count_scale_factor(image, density_map, yolo, yolo_image_processor, threshold):\n",
    "    with torch.no_grad():\n",
    "        num_of_objects = run_yolo(yolo, yolo_image_processor, image, config.clazz[:-1], threshold=threshold)\n",
    "        predicted_scale_factor = torch.sum(density_map / num_of_objects).item()\n",
    "        print(f\"YOLO found: {num_of_objects} objects, predicted scale factor is: {predicted_scale_factor}\")\n",
    "        return predicted_scale_factor\n",
    "\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.manual_seed(config.seed)\n",
    "torch.cuda.manual_seed(config.seed)\n",
    "\n",
    "# Counting model\n",
    "counting_model = utils.prepare_counting_model(config)\n",
    "clip, processor = utils.prepare_clip(config)\n",
    "if config.is_dynamic_scale_factor:\n",
    "    yolo = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
    "    yolo_image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    " \n",
    "exp_identifier = (\n",
    "    f'{config.epoch_size}_{config.lr}_'\n",
    "    f\"{config.seed}_{config.number_of_prompts}_{config.early_stopping}\"\n",
    ")\n",
    "\n",
    "#### Train ####\n",
    "print(f\"Start experiment {exp_identifier}\")\n",
    "\n",
    "class_name = f\"{config.amount} {config.clazz}\"\n",
    "print(f\"Start training class token for {class_name}\")\n",
    "img_dir_path = f\"img/{class_name}/train\"\n",
    "if Path(img_dir_path).exists():\n",
    "    shutil.rmtree(img_dir_path)\n",
    "Path(img_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/sdxl-turbo\",\n",
    "    torch_dtype=torch.float32\n",
    ").to(device)\n",
    "\n",
    "unet, vae, text_encoder, scheduler, tokenizer = pipeline.unet, pipeline.vae, pipeline.text_encoder, pipeline.scheduler, pipeline.tokenizer\n",
    "\n",
    "#  Extend tokenizer and add a discriminative token ###\n",
    "class_infer = int(class_name.split()[0])\n",
    "prompt_suffix = \" \".join(class_name.lower().split(\"_\"))\n",
    "\n",
    "## Add the placeholder token in tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(config.placeholder_token)\n",
    "if num_added_tokens == 0:\n",
    "    raise ValueError(\n",
    "        f\"The tokenizer already contains the token {config.placeholder_token}. Please pass a different\"\n",
    "        \" `placeholder_token` that is not already in the tokenizer.\"\n",
    "    )\n",
    "\n",
    "## Get token ids for our placeholder and initializer token.\n",
    "# This code block will complain if initializer string is not a single token\n",
    "## Convert the initializer_token, placeholder_token to ids\n",
    "token_ids = tokenizer.encode(config.initializer_token, add_special_tokens=False)\n",
    "# Check if initializer_token is a single token or a sequence of tokens\n",
    "if len(token_ids) > 1:\n",
    "    raise ValueError(\"The initializer token must be a single token.\")\n",
    "\n",
    "initializer_token_id = token_ids[0]\n",
    "placeholder_token_id = tokenizer.convert_tokens_to_ids(config.placeholder_token)\n",
    "\n",
    "# we resize the token embeddings here to account for placeholder_token\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#  Initialise the newly added placeholder token\n",
    "token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]\n",
    "\n",
    "# Define dataloades\n",
    "\n",
    "def collate_fn(examples):\n",
    "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "    input_ids = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    texts = [example[\"instance_prompt\"] for example in examples]\n",
    "    batch = {\n",
    "        \"texts\": texts,\n",
    "        \"input_ids\": input_ids,\n",
    "    }\n",
    "\n",
    "    return batch\n",
    "\n",
    "train_dataset = prompt_dataset.PromptDataset(\n",
    "    prompt_suffix=prompt_suffix,\n",
    "    tokenizer=tokenizer,\n",
    "    placeholder_token=config.placeholder_token,\n",
    "    number_of_prompts=config.number_of_prompts,\n",
    "    epoch_size=config.epoch_size,\n",
    ")\n",
    "\n",
    "train_batch_size = config.batch_size\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Define optimization\n",
    "\n",
    "## Freeze vae and unet\n",
    "utils.freeze_params(vae.parameters())\n",
    "utils.freeze_params(unet.parameters())\n",
    "\n",
    "## Freeze all parameters except for the token embeddings in text encoder\n",
    "params_to_freeze = itertools.chain(\n",
    "    text_encoder.text_model.encoder.parameters(),\n",
    "    text_encoder.text_model.final_layer_norm.parameters(),\n",
    "    text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
    ")\n",
    "utils.freeze_params(params_to_freeze)\n",
    "\n",
    "optimizer_class = torch.optim.AdamW\n",
    "optimizer = optimizer_class(\n",
    "    text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings\n",
    "    lr=config.lr,\n",
    "    betas=config.betas,\n",
    "    weight_decay=config.weight_decay,\n",
    "    eps=config.eps,\n",
    ")\n",
    "criterion = torch.nn.L1Loss().cuda()\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    mixed_precision=config.mixed_precision,\n",
    ")\n",
    "\n",
    "if config.gradient_checkpointing:\n",
    "    text_encoder.gradient_checkpointing_enable()\n",
    "    unet.enable_gradient_checkpointing()\n",
    "\n",
    "text_encoder, optimizer, train_dataloader = accelerator.prepare(\n",
    "    text_encoder, optimizer, train_dataloader\n",
    ")\n",
    "\n",
    "weight_dtype = torch.float32\n",
    "if accelerator.mixed_precision == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "elif accelerator.mixed_precision == \"bf16\":\n",
    "    weight_dtype = torch.bfloat16\n",
    "\n",
    "# Move vae and unet to device\n",
    "vae.to(accelerator.device, dtype=weight_dtype)\n",
    "unet.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "counting_model = counting_model.to(accelerator.device)\n",
    "text_encoder = text_encoder.to(accelerator.device)\n",
    "\n",
    "# Keep vae in eval mode as we don't train it\n",
    "vae.eval()\n",
    "# Keep unet in train mode to enable gradient checkpointing\n",
    "unet.train()\n",
    "\n",
    "global_step = 0\n",
    "total_loss = 0\n",
    "min_loss = 99999\n",
    "\n",
    "# Define token output dir\n",
    "token_dir_path = f\"token/{class_name}\"\n",
    "token_path = f\"{token_dir_path}/{exp_identifier}_{class_name}\"\n",
    "Path(token_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#### Training loop ####\n",
    "train_start = time.time()\n",
    "for epoch in range(config.num_train_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    generator = torch.Generator(\n",
    "        device=config.device\n",
    "    )  # Seed generator to create the inital latent noise\n",
    "    generator.manual_seed(config.seed)\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        step_start = time.time()\n",
    "        # setting the generator here means we update the same images\n",
    "        classification_loss = None\n",
    "        with accelerator.accumulate(text_encoder):\n",
    "\n",
    "            generator.manual_seed(config.seed)\n",
    "            \n",
    "            t1 = time.time()\n",
    "            # generate image            \n",
    "            image = pipeline(prompt=batch['texts'][0],\n",
    "                num_inference_steps=1,\n",
    "                output_type=\"pt\",\n",
    "                height=config.height,\n",
    "                width=config.width,\n",
    "                generator=generator,\n",
    "                guidance_scale=0.0\n",
    "            ).images[0] \n",
    "            print(f\"SDXL took {(time.time()-t1)/60} minutes\")\n",
    "            \n",
    "            image = image.unsqueeze(0)\n",
    "            image_out = image\n",
    "            image = utils.transform_img_tensor(image, config).to(device)\n",
    "            \n",
    "            prompt = [class_name.split()[-1]]\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                orig_output = counting_model(image, prompt)\n",
    "            \n",
    "            scale_factor = extract_clip_count_scale_factor(image_out.detach(), orig_output[0].detach(), yolo, yolo_image_processor, config.yolo_threshold) if config.is_dynamic_scale_factor else config.scale\n",
    "            output = torch.sum(orig_output[0] / scale_factor)\n",
    "\n",
    "            if classification_loss is None:\n",
    "                classification_loss = criterion(\n",
    "                    output, torch.HalfTensor([class_infer]).cuda()\n",
    "                )/torch.HalfTensor([1]).cuda() # TODO removed power 2\n",
    "            else:\n",
    "                classification_loss += criterion(\n",
    "                    output, torch.HalfTensor([class_infer]).cuda()\n",
    "                )/torch.HalfTensor([1]).cuda() # TODO removed power 2\n",
    "                            \n",
    "            text_inputs = processor(text=prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "            inputs = {**text_inputs, \"pixel_values\": image}\n",
    "            clip_output = (clip(**inputs)[0][0]/100).cuda()\n",
    "            clip_output = _lambda * (1-clip_output)\n",
    "\n",
    "            classification_loss += clip_output\n",
    "\n",
    "            total_loss += classification_loss.detach().item()\n",
    "\n",
    "            # log\n",
    "            txt = f\"On epoch {epoch} \\n\"\n",
    "            with torch.no_grad():\n",
    "                txt += f\"{batch['texts']} \\n\"\n",
    "                txt += f\"{output.item()=} \\n\"\n",
    "                txt += f\"Loss: {classification_loss.detach().item()} \\n\"\n",
    "                txt += f\"Clip-Count loss: {classification_loss.detach().item()-clip_output.detach().item()} \\n\"\n",
    "                txt += f\"Clip loss: {clip_output.detach().item()}\"\n",
    "                with open(\"run_log.txt\", \"a\") as f:\n",
    "                    print(txt, file=f)\n",
    "                print(txt)\n",
    "                display(utils.numpy_to_pil(\n",
    "                    image_out.detach().permute(0, 2, 3, 1).cpu().numpy()\n",
    "                )[0])\n",
    "                                \n",
    "                # counting prediction heatmap\n",
    "                pred_density = orig_output[0].detach().cpu().numpy()\n",
    "                pred_density = pred_density/pred_density.max()\n",
    "                pred_density_write = 1. - pred_density\n",
    "                pred_density_write = cv2.applyColorMap(np.uint8(255*pred_density_write), cv2.COLORMAP_JET)\n",
    "                pred_density_write = pred_density_write/255.\n",
    "                img = TF.resize(image.detach(), (384)).squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "                heatmap_pred = 0.33 * img + 0.67 * pred_density_write\n",
    "                heatmap_pred = heatmap_pred/heatmap_pred.max()\n",
    "                display(utils.numpy_to_pil(\n",
    "                    heatmap_pred\n",
    "                )[0])\n",
    "                \n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                text_encoder.get_input_embeddings().parameters(),\n",
    "                config.max_grad_norm,\n",
    "            )\n",
    "\n",
    "            start_back = time.time()\n",
    "            \n",
    "            accelerator.backward(classification_loss)\n",
    "            \n",
    "            # Zero out the gradients for all token embeddings except the newly added\n",
    "            # embeddings for the concept, as we only want to optimize the concept embeddings\n",
    "            if accelerator.num_processes > 1:\n",
    "                grads = (\n",
    "                    text_encoder.module.get_input_embeddings().weight.grad\n",
    "                )\n",
    "            else:\n",
    "                grads = text_encoder.get_input_embeddings().weight.grad\n",
    "\n",
    "            # Get the index for tokens that we want to zero the grads for\n",
    "            index_grads_to_zero = (\n",
    "                torch.arange(len(tokenizer)) != placeholder_token_id\n",
    "            )\n",
    "            grads.data[index_grads_to_zero, :] = grads.data[\n",
    "                index_grads_to_zero, :\n",
    "            ].fill_(0)\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\\n\",\n",
    "            if step == config.epoch_size - 1:\n",
    "                if total_loss > 2 * min_loss:\n",
    "                    print(\"!!!!training collapse, try different hp!!!!\")\n",
    "                    # epoch = config.num_train_epochs\n",
    "                    # break\n",
    "                print(\"update\")\n",
    "                if total_loss < min_loss:\n",
    "                    min_loss = total_loss\n",
    "                    current_early_stopping = config.early_stopping\n",
    "                    # Create the pipeline using the trained modules and save it.\n",
    "                    accelerator.wait_for_everyone()\n",
    "                    if accelerator.is_main_process:\n",
    "                        token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "                        torch.save(token_embeds[placeholder_token_id], f\"{token_path}/token_embeds.pt\")\n",
    "                        print(f\"Saved the new discriminative class token pipeline of {class_name} to pipeline_{token_path}\")\n",
    "                else:\n",
    "                    current_early_stopping -= 1\n",
    "                print(\n",
    "                    f\"{current_early_stopping} steps to stop, current best {min_loss}\"\n",
    "                )\n",
    "\n",
    "                total_loss = 0\n",
    "                global_step += 1\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    if current_early_stopping < 0:\n",
    "        break"
   ],
   "metadata": {
    "id": "ASKlY8U83LdM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e323fc445422434a88336c4eeb5d423b",
      "b79a3b4a2ed84b15828ad59b029a5fdd",
      "d2fc80860b234546bc5c0067c585fe91",
      "2c1bcd22eedd4c67be4e962005482dfd",
      "36b4457dc0614826882aa714f02dcfe4",
      "eb6a1537f6f1401a9d95f8f665440daf",
      "685f8f88ebed426f97a8d1a3c3799b5e",
      "874f536d05074145af6a1f866ccfc80d",
      "b71b767e97e844c1bd3aed1c3a9a456e",
      "379887bc2d3b4547a9bca6fea6071db2",
      "094033e01e174fd1881f64d4f5359f00"
     ]
    },
    "outputId": "d191037d-b38b-4590-876c-6596222c14b1",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Eval"
   ],
   "metadata": {
    "id": "DRWHrWCOQtHS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Evaluation - print image without discriminatory tokens, then one with.\")\n",
    "\n",
    "generator = torch.Generator(device=config.device)  # Seed generator to create the initial latent noise\n",
    "generator.manual_seed(config.seed)\n",
    "\n",
    "for descriptive_token in [\"some\", config.placeholder_token]:\n",
    "  generator.manual_seed(config.seed)\n",
    "  prompt = f\"A photo of {descriptive_token} {class_name}\"\n",
    "  print(f\"Evaluation for the prompt: {prompt}\")\n",
    "\n",
    "  image = pipeline(prompt=prompt,\n",
    "        num_inference_steps=1,\n",
    "        output_type=\"pt\",\n",
    "        height=config.height,\n",
    "        width=config.width,\n",
    "        generator=generator,\n",
    "        guidance_scale=0.0\n",
    "  ).images[0] \n",
    "  \n",
    "  display(utils.numpy_to_pil(image.permute(1, 2, 0).cpu().detach().numpy())[0])"
   ],
   "metadata": {
    "id": "ZIXFnn1LAkTh",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
